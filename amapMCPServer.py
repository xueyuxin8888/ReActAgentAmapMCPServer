import os
import asyncio

from click import command
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_core.messages import SystemMessage, HumanMessage
from langchain.chat_models import init_chat_model
from typing import Dict, List, Any
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            model="qwen-max",
            temperature=0.5,
            timeout=30,  # æ·»åŠ è¶…æ—¶é…ç½®ï¼ˆç§’ï¼‰
            max_retries=2  # æ·»åŠ é‡è¯•æ¬¡æ•°
        )
# è·å–é«˜å¾·åœ°å›¾ API Key
AMAP_MAPS_API_KEY=os.getenv('AMAP_MAPS_API_KEY')

# ä½¿ç”¨ langgraph æ¨èæ–¹å¼å®šä¹‰å¤§æ¨¡å‹
# ä¼ä¸šéƒ½ä¼šä½¿ç”¨æœ¬åœ°ç§æœ‰åŒ–éƒ¨ç½²æ¨¡å‹ï¼ˆæ•°æ®å®‰å…¨ï¼‰
# llm = init_chat_model(
#     model="deepseek-chat",
#     temperature=0,
#     model_provider="deepseek",
# )


# è§£ææ¶ˆæ¯åˆ—è¡¨
def parse_messages(messages: List[Any]) -> None:
    """
    è§£ææ¶ˆæ¯åˆ—è¡¨ï¼Œæ‰“å° HumanMessageã€AIMessage å’Œ ToolMessage çš„è¯¦ç»†ä¿¡æ¯

    Args:
        messages: åŒ…å«æ¶ˆæ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯æ˜¯ä¸€ä¸ªå¯¹è±¡
    """
    print("=== æ¶ˆæ¯è§£æç»“æœ ===")
    for idx, msg in enumerate(messages, 1):
        print(f"\næ¶ˆæ¯ {idx}:")
        # è·å–æ¶ˆæ¯ç±»å‹
        msg_type = msg.__class__.__name__
        print(f"ç±»å‹: {msg_type}")
        # æå–æ¶ˆæ¯å†…å®¹
        content = getattr(msg, 'content', '')
        print(f"å†…å®¹: {content if content else '<ç©º>'}")
        # å¤„ç†é™„åŠ ä¿¡æ¯
        additional_kwargs = getattr(msg, 'additional_kwargs', {})
        if additional_kwargs:
            print("é™„åŠ ä¿¡æ¯:")
            for key, value in additional_kwargs.items():
                if key == 'tool_calls' and value:
                    print("  å·¥å…·è°ƒç”¨:")
                    for tool_call in value:
                        print(f"    - ID: {tool_call['id']}")
                        print(f"      å‡½æ•°: {tool_call['function']['name']}")
                        print(f"      å‚æ•°: {tool_call['function']['arguments']}")
                else:
                    print(f"  {key}: {value}")
        # å¤„ç† ToolMessage ç‰¹æœ‰å­—æ®µ
        if msg_type == 'ToolMessage':
            tool_name = getattr(msg, 'name', '')
            tool_call_id = getattr(msg, 'tool_call_id', '')
            print(f"å·¥å…·åç§°: {tool_name}")
            print(f"å·¥å…·è°ƒç”¨ ID: {tool_call_id}")
        # å¤„ç† AIMessage çš„å·¥å…·è°ƒç”¨å’Œå…ƒæ•°æ®
        if msg_type == 'AIMessage':
            tool_calls = getattr(msg, 'tool_calls', [])
            if tool_calls:
                print("å·¥å…·è°ƒç”¨:")
                for tool_call in tool_calls:
                    print(f"  - åç§°: {tool_call['name']}")
                    print(f"    å‚æ•°: {tool_call['args']}")
                    print(f"    ID: {tool_call['id']}")
            # æå–å…ƒæ•°æ®
            metadata = getattr(msg, 'response_metadata', {})
            if metadata:
                print("å…ƒæ•°æ®:")
                token_usage = metadata.get('token_usage', {})
                print(f"  ä»¤ç‰Œä½¿ç”¨: {token_usage}")
                print(f"  æ¨¡å‹åç§°: {metadata.get('model_name', 'æœªçŸ¥')}")
                print(f"  å®ŒæˆåŸå› : {metadata.get('finish_reason', 'æœªçŸ¥')}")
        # æ‰“å°æ¶ˆæ¯ ID
        msg_id = getattr(msg, 'id', 'æœªçŸ¥')
        print(f"æ¶ˆæ¯ ID: {msg_id}")
        print("-" * 50)


# ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
def save_graph_visualization(graph, filename: str = "graph.png") -> None:
    """ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤ºã€‚

    Args:
        graph: çŠ¶æ€å›¾å®ä¾‹ã€‚
        filename: ä¿å­˜æ–‡ä»¶è·¯å¾„ã€‚
    """
    # å°è¯•æ‰§è¡Œä»¥ä¸‹ä»£ç å—
    try:
        # ä»¥äºŒè¿›åˆ¶å†™æ¨¡å¼æ‰“å¼€æ–‡ä»¶
        with open(filename, "wb") as f:
            # å°†çŠ¶æ€å›¾è½¬æ¢ä¸ºMermaidæ ¼å¼çš„PNGå¹¶å†™å…¥æ–‡ä»¶
            f.write(graph.get_graph().draw_mermaid_png())
        # è®°å½•ä¿å­˜æˆåŠŸçš„æ—¥å¿—
        print(f"Graph visualization saved as {filename}")
    # æ•è·IOé”™è¯¯
    except IOError as e:
        # è®°å½•è­¦å‘Šæ—¥å¿—
        print(f"Failed to save graph visualization: {e}")


# å®šä¹‰å¹¶è¿è¡Œagent
async def run_agent():
    # å®ä¾‹åŒ– MCP Serverå®¢æˆ·ç«¯
    client = MultiServerMCPClient({
        # é«˜å¾·åœ°å›¾ MCP Server
        # "amap-amap-sse": {
        #     "url": "https://mcp.amap.com/sse?key="+AMAP_MAPS_API_KEY,
        #     "transport": "sse",
        # },
        "amap-maps-streamableHTTP": {
            "url": "https://mcp.amap.com/mcp?key=" + AMAP_MAPS_API_KEY,
            "transport": "streamable_http",
        },
        # è‡ªå®šä¹‰ MCP Server
        # "calculator": {
        #     "command": "python",
        #     "args": ["calculatorMCPServer.py"],
        #     "transport": "stdio"
        # },
        "calculator": {
            "url": "http://127.0.0.1:8000/mcp",
            "transport": "streamable_http"
        },
    #     "tavily-remote-mcp": {
    #         "command": "cmd",
    #         "args": ["/c",
    #                  "npx -y mcp-remote https://mcp.tavily.com/mcp/?tavilyApiKey=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"],
    #         "env": {},
    #         "transport": "streamable_http"
    # }
        "tavily-remote-mcp": {
            "url": "https://mcp.tavily.com/mcp/?tavilyApiKey=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",
            "transport": "streamable_http"
        }
    })

    # ä»MCP Serverä¸­è·å–å¯æä¾›ä½¿ç”¨çš„å…¨éƒ¨å·¥å…·
    tools = await client.get_tools()
    # print(f"tools:{tools}\n")

    # åŸºäºå†…å­˜å­˜å‚¨çš„short-term
    checkpointer = InMemorySaver()

    # å®šä¹‰ç³»ç»Ÿæ¶ˆæ¯ï¼ŒæŒ‡å¯¼å¦‚ä½•ä½¿ç”¨å·¥å…·
    system_message = SystemMessage(content=(
        "ä½ æ˜¯ä¸€ä¸ªæ—…è¡Œè§„åˆ’åŠ©æ‰‹ï¼Œä½ å¯ä»¥ä½¿ç”¨é«˜å¾·åœ°å›¾å·¥å…·è·å–å‡ºè¡Œå’Œå¤©æ°”ç­‰ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨tavilyå·¥å…·æœç´¢ç½‘é¡µä¿¡æ¯å¸®åŠ©è¿›è¡Œæ—…è¡Œè§„åˆ’ï¼Œè¿˜å¯ä»¥ä½¿ç”¨è®¡ç®—å™¨å·¥å…·ä¿éšœè®¡ç®—é¢„ç®—çš„æ­£ç¡®æ€§ã€‚"
    ))

    # åˆ›å»º ReActé£æ ¼çš„ agent
    agent = create_react_agent(
        model=llm,
        tools=tools,
        prompt=system_message,
        checkpointer=checkpointer
    )

    # å°†å®šä¹‰çš„agentçš„graphè¿›è¡Œå¯è§†åŒ–è¾“å‡ºä¿å­˜è‡³æœ¬åœ°
    # save_graph_visualization(agent)

    # å®šä¹‰short-terméœ€ä½¿ç”¨çš„thread_id
    config = {"configurable": {"thread_id": "1"}}

    # 1ã€éæµå¼å¤„ç†æŸ¥è¯¢
    # é«˜å¾·åœ°å›¾æ¥å£æµ‹è¯•
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="è¿™ä¸ª114.05571,22.52245ç»çº¬åº¦å¯¹åº”çš„åœ°æ–¹æ˜¯å“ªé‡Œ")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æ·±åœ³çº¢æ ‘æ—çš„ç»çº¬åº¦åæ ‡æ˜¯å¤šå°‘")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="112.10.22.229è¿™ä¸ªIPæ‰€åœ¨ä½ç½®")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æ·±åœ³çš„å¤©æ°”å¦‚ä½•")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æˆ‘è¦ä»æ·±åœ³å¸‚å—å±±åŒºä¸­å…´å¤§å¦éª‘è¡Œåˆ°å®å®‰åŒºå®å®‰ä½“è‚²é¦†ï¼Œå¸®æˆ‘è§„åˆ’ä¸‹è·¯å¾„")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æˆ‘è¦ä»æ·±åœ³å¸‚å—å±±åŒºä¸­å…´å¤§å¦æ­¥è¡Œåˆ°å®å®‰åŒºå®å®‰ä½“è‚²é¦†ï¼Œå¸®æˆ‘è§„åˆ’ä¸‹è·¯å¾„")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æˆ‘è¦ä»æ·±åœ³å¸‚å—å±±åŒºä¸­å…´å¤§å¦é©¾è½¦åˆ°å®å®‰åŒºå®å®‰ä½“è‚²é¦†ï¼Œå¸®æˆ‘è§„åˆ’ä¸‹è·¯å¾„")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æˆ‘è¦ä»æ·±åœ³å¸‚å—å±±åŒºä¸­å…´å¤§å¦åå…¬å…±äº¤é€šåˆ°å®å®‰åŒºå®å®‰ä½“è‚²é¦†ï¼Œå¸®æˆ‘è§„åˆ’ä¸‹è·¯å¾„")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æµ‹é‡ä¸‹ä»æ·±åœ³å¸‚å—å±±åŒºä¸­å…´å¤§å¦åˆ°å®å®‰åŒºå®å®‰ä½“è‚²é¦†é©¾è½¦è·ç¦»æ˜¯å¤šå°‘")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æ·±åœ³å¸‚å—å±±åŒºä¸­çŸ³åŒ–çš„åŠ æ²¹ç«™æœ‰å“ªäº›ï¼Œéœ€è¦æœ‰POIçš„ID")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="POIä¸ºB020016GPHçš„è¯¦ç»†ä¿¡æ¯")]}, config)
    # agent_response = await agent.ainvoke({"messages": [HumanMessage(content="æ·±åœ³å¸‚å—å±±åŒºå‘¨å›´10å…¬é‡Œçš„ä¸­çŸ³åŒ–çš„åŠ æ²¹ç«™")]}, config)
    # å°†è¿”å›çš„messagesè¿›è¡Œæ ¼å¼åŒ–è¾“å‡º
    # parse_messages(agent_response['messages'])
    # agent_response_content = agent_response["messages"][-1].content
    # print(f"agent_response:{agent_response_content}")


    # 2ã€æµå¼å¤„ç†æŸ¥è¯¢
    # async for message_chunk, metadata in agent.astream(
    #         input={"messages": [HumanMessage(content="3ä¹˜ä»¥4ç­‰äºå¤šå°‘")]},
    #         config=config,
    #         stream_mode="messages"
    # ):
    #     # æµ‹è¯•åŸå§‹è¾“å‡º
    #     # print(f"Token:{message_chunk}\n")
    #     # print(f"Metadata:{metadata}\n\n")
    #
    #     # è·³è¿‡å·¥å…·è¾“å‡º
    #     if metadata["langgraph_node"]=="tools":
    #         continue
    #
    #     # è¾“å‡ºæœ€ç»ˆç»“æœ
    #     if message_chunk.content:
    #         print(message_chunk.content, end="", flush=True)
    while True:
        user_input = input("\nğŸ§  è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š").strip()
        if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
            print("ğŸ‘‹ å·²é€€å‡ºã€‚")
            break

        # å¯åŠ¨å¼‚æ­¥æµå¼è°ƒç”¨
        async for message_chunk, metadata in agent.astream(
                input={"messages": [HumanMessage(content=user_input)]},
                config=config,
                stream_mode="messages"
        ):
            # è·³è¿‡å·¥å…·è¾“å‡º
            if metadata["langgraph_node"] == "tools":
                continue

            # è¾“å‡ºæœ€ç»ˆç»“æœï¼ˆä¸æ¢è¡Œï¼‰
            if message_chunk.content:
                print(message_chunk.content, end="", flush=True)

        print()  # æ¯è½®å¯¹è¯ç»“æŸæ¢è¡Œ


if __name__ == "__main__":
    asyncio.run(run_agent())




